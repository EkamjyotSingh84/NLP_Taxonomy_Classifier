{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5tr_jEBnh-jv"
   },
   "source": [
    "# Title:\n",
    "\n",
    "#### Group Member Names :\n",
    "Ekamjyot Singh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PeKSxMvrh-j0"
   },
   "source": [
    "### INTRODUCTION:\n",
    "Exploring the Landscape of Natural Language Processing Research\n",
    "\n",
    "#### AIM :\n",
    "The core requirement of the assignment is to implement and evaluate the described NLP Taxonomy Classifier in the referenced paper. This means making sense of how the model was designed, training it on a dataset, and measuring the performance of the model with respect to the defined taxonomy while classifying NLP research papers. We are specifically working toward assessing the model's accuracy, precision, and recall, besides the possible improvements or extensions.\n",
    "\n",
    "#### Github Repo:\n",
    "Exploring-NLP-Research GitHub Repository\n",
    "(https://github.com/sebischair/Exploring-NLP-Research?tab=readme-ov-file)\n",
    "\n",
    "#### DESCRIPTION OF PAPER:\n",
    "The paper presents a fine-tuned BERT-based model for the classification of NLP research papers with respect to a finely grained taxonomy. The taxonomy includes different levels of concepts about NLP that will let a model predict both specific and hierarchical concepts. It has been trained on a large dataset of 178,521 papers from ACL Anthology, arXiv cs.CL domain, and Scopus. The model was trained by initialization with the weights from allennlp/transformer-specter2-base and fine-tuning on a weakly labeled dataset. The approach can be said to improve the model's ability to accurately classify papers across a myriad of concepts in NLP.\n",
    "\n",
    "#### PROBLEM STATEMENT :\n",
    "With rapid advancement and diversification, NLP research has come of age. Currently, methods for their classification and analysis are usually lacking in structure due to an overwhelming number of scientific papers. There lies a lacking framework that can let one understand the trends and gaps in the research. There is a pressing need for a robust classification system that will help in systematic categorization on a well-defined taxonomy.\n",
    "\n",
    "#### CONTEXT OF THE PROBLEM:\n",
    "NLP research is progressing very fast, with new methods, models, and applications constantly appearing. The lack of a comprehensive, well-structured classification system might to some extent confuse understanding of the extent of all contributions. A correctly defined taxonomy offers clarity, permits the recognition of tendencies within research, and thus enables researchers and practitioners to be more informative about recent developments and key focus areas in NLP.\n",
    "\n",
    "#### SOLUTION:\n",
    "The proposed solution will involve development of fine-tuned language model based on BERT for classification of NLP research paper. This taxonomy requires predicting the concepts of multi-levels toward a multi-label classification task. The model should have been built training on big data and fine-tuned with domain-specific data for the ability to categorize the document with high accuracy and to identify the paper with the relevant NLP concepts. The answer responds to the demand for a systematic and effective categorization system by giving the real skeletal frame for the analysis of NLP research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77PIPLQ-h-j1"
   },
   "source": [
    "# Background\n",
    "Reference: Schopf, T., Arabi, K., Matthes, F. (2023). On the Landscape of Natural Language Processing Research. Proceedings of the 14th International Conference on Recent Advances in Natural Language Processing (RANLP 2023).\r\n",
    "Explanation: \n",
    "The paper describes a new method to classify NLP research papers with the use of a fine-tuned BERT model. It provides a highly detailed taxonomy that will help classify the paper based on its research topics and contributions.\n",
    "\r\n",
    "Dataset/Input: The model was trained on a dataset comprising 178,521 papers drawn from ACL Anthology, arXiv cs.CL domain, and Scopus. In essence, it comprises all forms of research articles concerning NLP and thus effectively covers this broad sphere for this particular subject\n",
    "Weakness: .\r\n",
    "Nevertheless, the model may still be weak concerning the robustness of its performance against varying content and taxonomy coverage across the papers. Furthermore, challenges in classifying papers that lie beyond predefined taxonomy or reflect new trends in research not well represented in the training data may emer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deODH3tMh-j2"
   },
   "source": [
    "# Implement paper code :\n",
    "from typing import List\r\n",
    "import torch\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "from transformers import BertForSequenceClassification, AutoTokeni\n",
    "zer\r\n",
    "# load model and tokenizer\r\n",
    "tokenizer = AutoTokenizer.from_pretrained('TimSchopf/nlp_taxonomy_classifier')\r\n",
    "model = BertForSequenceClassification.from_pretrained('TimSchopf/nlp_taxonomy_classifier')\r\n",
    "\r\n",
    "# prepare data\r\n",
    "papers = [{'title': 'Attention Is All You Need', 'abstract': 'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.'},\r\n",
    "          {'title': 'SimCSE: Simple Contrastive Learning of Sentence Embeddings', 'abstract': 'This paper presents SimCSE, a simple contrastive learning framework that greatly advances state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation, and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework by using \"entailment\" pairs as positives and \"contradiction\" pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3% and 81.6% Spearmans correlation respectively, a 4.2% and 2.2% improvement compared to the previous best results. We also show -- both theoretically and empirically -- that the contrastive learning objective regularizes pre-trained embeddings anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are ava\n",
    "          ilable.'}]\r\n",
    "# concatenate title and abstract with [SEP] token\r\n",
    "title_abs = [d['title'] + tokenizer.sep_token + (d.get('abstract') or '') for d in papers]\r\n",
    "\r\n",
    "\r\n",
    "def predict_nlp_concepts(model, tokenizer, texts: List[str], batch_size=8, device=None, shuffle_data=False):\r\n",
    "    \"\"\"\r\n",
    "    helper function for predicting NLP concepts of scientific pap\n",
    "    \"\"\"\r\n",
    "    \r\n",
    "    # tokenize texts\r\n",
    "    def tokenize_dataset(sentences, tokenizer):\r\n",
    "        sentences_num = len(sentences)\r\n",
    "        dataset = []\r\n",
    "        for i in range(sentences_num):\r\n",
    "            \r\n",
    "            sentence = tokenizer(sentences[i], padding=\"max_length\", truncation=True, return_tensors='pt', max_length=model.config.max_position\r\n",
    "            \r\n",
    "            # get input_ids, token_type_ids, and attention_mask\r\n",
    "            input_ids = sentence['input_ids'][0]\r\n",
    "            token_type_ids = sentence['token_type_ids'][0]\r\n",
    "            attention_mask = sentence['attention_mask'][0]\r\n",
    "\r\n",
    "            dataset.append((input_ids, token_type_ids, attention_mask))\r\n",
    "        return dataset\r\n",
    "\r\n",
    "    tokenized_data = tokenize_dataset(sentencexts, tokenizer=tokenizer)\r\n",
    "    \r\n",
    "    # get the individual input formats for the model\r\n",
    "    input_ids = torch.stack([x[0] for x in tokenized_data])\r\n",
    "    token_type_ids = torch.stack([x[1] for x in tokenized_data])\r\n",
    "    attention_mask_ids = torch.stack([x[2].to(torloat) for x in tokenized_data])\r\n",
    "    \r\n",
    "    # convert input to DataLoader\r\n",
    "    input_dataset = []\r\n",
    "    for i in range(len(input_ids)):\r\n",
    "        data = {}\r\n",
    "        data['input_ids'] = input_ids[i]\r\n",
    "        data['token_type_ids'] = token_type_ids[i]\r\n",
    "        data['attention_mask'] = attention_mask_ids[i]\r\n",
    "        input_dataset.append(data)\r\n",
    "\r\n",
    "    dataloader = DataLoader(input_datasetuffle=shuffle_data, batch_size=batch_size)\r\n",
    "    \r\n",
    "    # predict data\r\n",
    "    if not device:\r\n",
    "        device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\r\n",
    "\r\n",
    "    model.to(device)\r\n",
    "    model.eval()\r\n",
    "    y_pred = torch.tensor([]).to(device)\r\n",
    "    for batch in dataloader:\r\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\r\n",
    "        input_ids_batch = batch['input_ids']\r\n",
    "        token_type_ids_batch = batch['token_type_ids']\r\n",
    "        mask_ids_batch = batch['attention_mask']\r\n",
    "\r\n",
    "        with torch.no_grad():\r\n",
    "            outputs = model(input_ids=input_ids_batch, attention_mask=mask_ids_batch, token_type_ids=token_type_ids_batch)\r\n",
    "\r\n",
    "        logits = outputs.logits\r\n",
    "        predictions = torch.round(torch.sigmoid(ls))\r\n",
    "        y_pred = torch.cat([y_pred,predictions])\r\n",
    "        \r\n",
    "    \r\n",
    "    # get prediction class names\r\n",
    "    prediction_indices_list = []\r\n",
    "    for prediction in y_pred:\r\n",
    "        prediction_indices_list.append((prediction == torch.max(prediction)).nonzero(as_tuple=True)[0])\r\n",
    "\r\n",
    "    prediction_class_names_list = []\r\n",
    "    for prediction_indices in prediction_indices_list:\r\n",
    "        prediction_class_names = []\r\n",
    "        for prediction_idx in prediction_indices:\r\n",
    "            prediction_class_names.append(model.config.id2label[int(prediction_idx)])\r\n",
    "        prediction_class_names_list.append(prediction_class_names)\r\n",
    "\r\n",
    "    return y_pred, prediction_class_names_list\r\n",
    "\r\n",
    "# predict concepts of NLP papers\r\n",
    "numerical_predictions, class_name_predictions = predict_nlp_concepts(model=model, tokenizer=tokenizer, texts=title_abs)izer.sep_token + (d.get('abstract') or '') for d in papers]\r\n",
    "\r\n",
    "pipe(title_abs, return_all_scores=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gkHhku9h-j2"
   },
   "source": [
    "### Contribution  Code :\n",
    "from transformers import BertForSequenceClassification, AutoTokenizer\r\n",
    "import torch\r\n",
    "from torch.utils.data import DataLoader, Dataset\r\n",
    "\r\n",
    "# Load model and tokenizer\r\n",
    "tokenizer = AutoTokenizer.from_pretrained('TimSchopf/nlp_taxonomy_classifier')\r\n",
    "model = BertForSequenceClassification.from_pretrained('TimSchopf/nlp_taxonomy_classifier')\r\n",
    "\r\n",
    "# Example Data\r\n",
    "[{'title': 'Attention Is All You Need', 'abstract': 'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.'},\n",
    "          {'title': 'SimCSE: Simple Contrastive Learning of Sentence Embeddings', 'abstract': 'This paper presents SimCSE, a simple contrastive learning framework that greatly advances state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation, and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework by using \"entailment\" pairs as positives and \"contradiction\" pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3% and 81.6% Spearmans correlation respectively, a 4.2% and 2.2% improvement compared to the previous best results. We also show -- both theoretically and empirically -- that the contrastive learning objective regularizes pre-trained embeddings anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available.'}]work...'}\r\n",
    "]\r\n",
    "\r\n",
    "# Function to preprocess data\r\n",
    "def preprocess_papers(papers):\r\n",
    "    # Concatenate title and abstract with a separator token\r\n",
    "    title_abs = [d['title'] + tokenizer.sep_token + (d.get('abstract') or '') for d in papers]\r\n",
    "    return title_abs\r\n",
    "\r\n",
    "# Convert papers to dataset format\r\n",
    "class PapersDataset(Dataset):\r\n",
    "    def __init__(self, papers, tokenizer, max_length=512):\r\n",
    "        self.papers = papers\r\n",
    "        self.tokenizer = tokenizer\r\n",
    "        self.max_length = max_length\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.papers)\r\n",
    "\r\n",
    "    def __getitem__(self, idx):\r\n",
    "        paper = self.papers[idx]\r\n",
    "        encoding = self.tokenizer(\r\n",
    "            paper,\r\n",
    "            truncation=True,\r\n",
    "            padding='max_length',\r\n",
    "            max_length=self.max_length,\r\n",
    "            return_tensors='pt'\r\n",
    "        )\r\n",
    "        return {\r\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\r\n",
    "            'attention_mask': encoding['attention_mask'].squeeze()\r\n",
    "        }\r\n",
    "\r\n",
    "# Function to predict NLP concepts\r\n",
    "def predict_nlp_concepts(model, tokenizer, papers, batch_size=8, device='cpu'):\r\n",
    "    # Prepare dataset and dataloader\r\n",
    "    dataset = PapersDataset(papers, tokenizer)\r\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\r\n",
    "    \r\n",
    "    model.to(device)\r\n",
    "    model.eval()\r\n",
    "    \r\n",
    "    predictions = []\r\n",
    "    with torch.no_grad():\r\n",
    "        for batch in dataloader:\r\n",
    "            input_ids = batch['input_ids'].to(device)\r\n",
    "            attention_mask = batch['attention_mask'].to(device)\r\n",
    "            \r\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\r\n",
    "            logits = outputs.logits\r\n",
    "            \r\n",
    "            # Get predicted labels (Assuming logits are probabilities)\r\n",
    "            preds = torch.sigmoid(logits).cpu().numpy()\r\n",
    "            predictions.extend(preds)\r\n",
    "    \r\n",
    "    return predictions\r\n",
    "\r\n",
    "# Extended functionality or improvements\r\n",
    "def extended_predict_nlp_concepts(model, tokenizer, papers, batch_size=8, device='cpu'):\r\n",
    "    # Preprocess papers and prepare dataset\r\n",
    "    title_abs = preprocess_papers(papers)\r\n",
    "    dataset = PapersDataset(title_abs, tokenizer)\r\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\r\n",
    "    \r\n",
    "    model.to(device)\r\n",
    "    model.eval()\r\n",
    "    \r\n",
    "    all_predictions = []\r\n",
    "    with torch.no_grad():\r\n",
    "        for batch in dataloader:\r\n",
    "            input_ids = batch['input_ids'].to(device)\r\n",
    "            attention_mask = batch['attention_mask'].to(device)\r\n",
    "            \r\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\r\n",
    "            logits = outputs.logits\r\n",
    "            \r\n",
    "            # Use a threshold for binary classification (e.g., 0.5)\r\n",
    "            preds = (torch.sigmoid(logits) > 0.5).cpu().numpy()\r\n",
    "            all_predictions.extend(preds)\r\n",
    "    \r\n",
    "    return all_predictions\r\n",
    "\r\n",
    "# Contribution Code\r\n",
    "def evaluate_model(predictions, true_labels):\r\n",
    "    \"\"\"\r\n",
    "    Evaluate the model's performance by calculating metrics like accuracy, precision, recall, and F1 score.\r\n",
    "    Args:\r\n",
    "    - predictions (list of numpy arrays): Model predictions\r\n",
    "    - true_labels (list of lists): True labels\r\n",
    "    \r\n",
    "    Returns:\r\n",
    "    - dict: Evaluation metrics\r\n",
    "    \"\"\"\r\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\r\n",
    "    \r\n",
    "    # Flatten lists for metric calculations\r\n",
    "    y_true = [label for sublist in true_labels for label in sublist]\r\n",
    "    y_pred = [pred for sublist in predictions for pred in sublist]\r\n",
    "    \r\n",
    "    metrics = {\r\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\r\n",
    "        'precision': precision_score(y_true, y_pred, average='weighted'),\r\n",
    "        'recall': recall_score(y_true, y_pred, average='weighted'),\r\n",
    "        'f1_score': f1_score(y_true, y_pred, average='weighted')\r\n",
    "    }\r\n",
    "    \r\n",
    "    return metrics\r\n",
    "\r\n",
    "# Example usage\r\n",
    "title_abs = preprocess_papers(papers)\r\n",
    "predictions = extended_predict_nlp_concepts(model, tokenizer, title_abs)\r\n",
    "true_labels = [[1, 0], [0, 1]]  # Replace with actual true labels\r\n",
    "metrics = evaluate_model(predictions, true_labels)\r\n",
    "\r\n",
    "print(\"Evaluation Metrics:\")\r\n",
    "print(f\"Accuracy: {metrics['accuracy']:.2f}\")\r\n",
    "print(f\"\n",
    "\n",
    "# Contributions:\r\n",
    "Data Preprocessing: Added a preprocess_papers function to preprocess paper data by concatenating the title and abstract with a separator token.\r\n",
    "Class Dataset: Implmented PapersDataset class to handle data efficiently and tokenise them.\r\n",
    "Batch Processing: Enhanced predict_nlp_concepts and extended_predict_nlp_concepts functions to support batch processing and prediction with thresholding.\r\n",
    "Evaluation Function: Added an evaluate_model function that will calculate the performance metrics, which include Accuracy, Precision, Recall, and the F1 Scre.\n",
    "recision: {metrics['precision']:.2f}\")\r\n",
    "print(f\"Recall: {metrics['recall']:.2f}\")\r\n",
    "print(f\"F1 Score: {metrics['f1_score']:.2f}\")\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YdFCgWoh-j3"
   },
   "source": [
    "### Results :\n",
    "Other evaluation metrics used in the NLP taxonomy classifier include the F1 score, recall, and precision. The model recorded an F1 score of 93.21, a recall of 93.99, and a precision of 92.46. This proves that the model is able to classify NLP research papers effectively and comprehensively.\n",
    "\n",
    "#### Observations :\n",
    "1)In that sense, it becomes an extremely effective model for the prediction of NLP concepts with great F1 score and precision.\n",
    "2)\n",
    "The high recall indicates that most of the relevant concepts are successfully identified, which provides very useful input for the process of classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3JVj9dKh-j3",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Conclusion and Future Direction :\n",
    "The classifier enhances the value of BERT-based NLP Taxonomy in the classification of NLP research papers. In turn, it allowed classification of the research papers in the NLP domain at the respective bottom levels, and a structured framework was given to NLP analysis. Expansion of future works on taxonomy includes emerging trends in NLP, e.g., possible research in other models or techniques for classification accuracy enhancement.\n",
    "\n",
    "#### Learnings :\n",
    "1)We have learned how to utilize BERT-based models for multi-label classification tasks.\r",
    "2)\n",
    "It was noted earlier that a well-defined taxonomy would have been a precondition for the organization and analysis of extensive research literature.\n",
    "\n",
    "#### Results Discussion :Results indicate that the model performs well in most classification tasks by maintaining high precision, high recall, and a high F1 score. These metrics may be read to mean that the model can be used to identify and categorize NLP concepts, and, therefore, will be helpful to a researcher or practitioner working in this area.\n",
    "*\n",
    "#### Limitations :1)The model could have problems with texts that are not perfectly adapted to the predefined taxonomy or with papers that present new concepts that did not appear in the training data.\r",
    "2)\n",
    "And the performance of the model can be varied due to the difference in quality and representativeness of the training dataset.**\n",
    "#### Future Extension 1)This may be further enhanced by an expansion of the taxonomy to cover new and emerging topics in NLP that seem relevant for the model's applicability.\r",
    "2)\n",
    "Other plausible models and techniques, such as those taking into account advanced neural architectures or other sources of information, might be able to enhance the performance of classification.:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATXtFdtBh-j4"
   },
   "source": [
    "# References:\n",
    "Schopf, T., Arabi, K., Matthes, F. (2023). Exploring the Landscape of Natural Language Processing Research. Proceedings of the 14th International Conference on Recent Advances in Natural Language Processing (RANLP 2023).\n",
    "(https://aclanthology.org/2023.ranlp-1.111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
